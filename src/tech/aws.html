<!DOCTYPE html>
<!--suppress CheckEmptyScriptTag -->
<html lang="en">
<head>
    <title>AWS Lessons</title>
    <include src="/fragment/head.html" />
</head>
<body>

<include src="/fragment/nav.html" />

<main>

    <h1>AWS Lessons</h1>

    <section>
        <h2>Multiple Accounts</h2>
        <h3>Avoid overlapping resources</h3>
        Many resources in AWS can be freely provisioned without worrying about overlap.
        But some resources require you to pick names that are unique to the AWS account.
        So if you have multiple environments in one AWS account, you'll have to carefully
        manage the names of everything in all of these services:
        <ul>
            <li>IAM (users, policies, roles)</li>
            <li>S3 (bucket names, globally unique)</li>
            <li>CloudFront (globally unique, I think?)</li>
        </ul>

        <h3>Limited Blast Radius for Teardowns</h3>
        If your account has both prod and non-prod resources,
        and especially if you're using Terraform,
        there is no way to comfortably (safely) destroy a terraform stack.
        IAM permissions lack the descriptive power to <mark>fully</mark> isolate resources based on IAM roles,
        so it's possible that something could go wrong with the destroy operation and spill over into production.

        <h3>Terraform multi-account challenge</h3>
        Terraform 0.11 (and maybe newer) requires the IAM user running the apply job to be
        in the same AWS account as the resources that are being terraformed, because some
        resources are loaded BEFORE the provider role is assumed.
    </section>

    <section>
        <h2>Misaligned timestamps in the AWS Console</h2>
        Some services present timestamps in browser time, others in UTC. Correlation is a huge pain.

        Workaround 1: Set your system time to UTC, and now all timestamps in AWS are consistent.
        You can still run an alternate clock in your menubar that still shows the local time.

        This menubar app is invaluable for dealing with inconsistent and braindead timestamps in AWS Console.
        https://apps.apple.com/ca/app/clocker/id1056643111?mt=12
    </section>

    <section>
        <h2>Site-to-Site VPN (AWS VPC)</h2>
        Okay, so the Site-to-Site VPN feature in AWS VPC is honestly just kinda bad.

        <h3>Troubleshooting is nigh-impossible</h3>
        Furthermore, AWS-managed VPN gives us basically no troubleshooting tools to identify the problem -- we have to rely entirely on our partners.
        The best I can do is watch some CloudWatch metrics with 1 minute resolution.

        <h3>Limited SA pairs</h3>
        <blockquote>
            Note: AWS accepts only a single pair of security associations for a VPN connection (one inbound and one outbound association)
            <a href="https://aws.amazon.com/premiumsupport/knowledge-center/vpn-tunnel-troubleshooting/">(source)</a>
        </blockquote>

        So if you're trying to link with a partner that wants to expose more than one subnet,
        especially two different hosts in wildly different subnets,
        there is a moderately good chance they'll try to use multiple security associations,
        and then you'll only ever be able to receive traffic from one of them.

        The workaround is to ask your partner to use route-based VPN on a vti.

        The real solution is to use a "real" VPN device like Cisco ASA or Palo Alto or whatever.
        Maybe even pfSense or StrongSwan would do better.

        <h3>Only accepts FIRST cipher proposal</h3>
        AWS apparently has a buggy implementation of their IPsec site-to-site VPN
        such that when a remote party (eg. Cisco ASA) is the initiator
        (NOTE: the remote party is ALWAYS the initiator)
        AWS will select the first cipher proposal instead of the best match.
        This causes Phase 1 negotiation to fail even if there are matching cipher suites available.

        This means that as long as we are using AWS VPN,
        we must tell our VPN partners that they have to configure a single cipher proposal.
    </section>

    <section>
        <h2>Network Load Balancer (EC2)</h2>
        <h3>No ICMP Echo (Ping)</h3>
        It is apparently impossible to ping an AWS Network Load Balancer.
        It won't respond on its own, and downstream targets don't have the option to reply to ICMP ping.
        This is a huge annoyance if you're using NLB with VPN, because VPN should have a ping-able target for keepalive.

        <blockquote>
            ICMP requests other than Type 3 are also considered unintended traffic.
            Network Load Balancers drop unintended traffic without forwarding it to any targets.
            <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html">(source)</a>
        </blockquote>

        <h3>No Security Groups</h3>
        Network Load Balancers (NLB) do not have security groups.
        CIDR-based access policy is actually dictated by the targets of the NLB.

        <h3>Limited Re-use</h3>
        Compared to an ALB with HTTP, network load balancers are not very reusable.
        HTTP 1.1 with Host headers and SNI allows multiple services to be fronted by a single ALB, awesome.
        But with an NLB, you get one application per port. THAT'S ALL.

        This gets even worse when you start mixing NLB with VPN connections.

        <h3>Even worse with ECS dynamic port mapping</h3>
        If you want to use an NLB internally between two ECS services,
        then you're going to run into dynamic port mapping which means you
        basically need to open up all ports for ingress/egress which
        eliminates all the good parts of security groups.
    </section>

    <section>
        <h2>Compute Instances (EC2)</h2>
        <h3>UserData limited to 16384 bytes</h3>
        If you're building immutable instances with EC2 Userdata, beware.
        Especially applies if you're populating a lot of SSH authorized_keys files.

        <blockquote>
            aws_instance.bastion: Error launching source instance: InvalidParameterValue: User data is limited to 16384 bytes
        </blockquote>

        Workaround: keep using Userdata, but copy large files from S3.
        Workaround: prepare your images using Packer.

        <h3>IMDS Instance metadata is risky</h3>
        TL;DR any software running on an EC2 instance with IMDSv1 can obtain AWS IAM credentials of the EC2 instance in the context of the owning AWS organization with a simple GET request. üò±

        IMDSv2 protects that resource by requiring an HTTP PUT before GET, which makes it much harder to abuse because most paths to abuse come through GET requests.
    </section>

    <section>
        <h2>Application Load Balancer (EC2)</h2>
        <h3>Hard to log content of failed requests</h3>
        No easy way to log failed requests to an ALB (4xx/5xx).
        You would need to mirror all traffic somewhere else
        And also set up detailed ALB logging
        And probably also VPC flow logs
        Then write something that could match up all of those together
        And do it quickly to avoid retaining a flood of data.

        AWS Support will tell you to do this yourself.
    </section>

    <section>
        <h2>Identity & Access Management (IAM)</h2>
        <h3>API keys are insecure by default</h3>
        By default, aws-sdk looks for your API keys in <code>~/.aws/credentials</code>.
        Dangerous! Because ANY process running in your user account can read that file.
        Anything you download or run from the internet could easily exfiltrate that API key.
        Your AWS API key can be leaked by any software you install on your Mac.

        <em>Workaround:</em> <a href="https://github.com/99designs/aws-vault">aws-vault</a>
        is very good. Definitely want to be using this for anyone who has highly
        privileged access to AWS, and probably also for anyone who uses an IAM key at all.

        Workaround: macOS 10.15 Catalina introduces new restrictions on file system access that makes it
        more difficult for malware to go digging through the hard drive for interesting gems like AWS keys.
        We could probably add another layer of defense in depth by upgrading to 10.15 Catalina
        and moving our AWS keys (and other secrets?) into the ~/Documents/ folder.

        <h3>Self Control</h3>
        There's no built-in policy that allows users to manage their own MFA, password, etc.
        <a href="aws-selfcontrol.json">IAM self-control policy (json)</a>

        <h3>Read-Only is Dangerous</h3>
        The read-only policy allows read access to contents of all buckets.
        If any of your data is sensitive, you should never grant read-only.
        What you USUALLY want is my structure-viewer policy.
        <a href="aws-selfcontrol.json">IAM structure-viewer policy (json)</a>

        <h3>AssumeRole is awesome</h3>
        AssumeRole changes out your current set of privileges for a different set.
        Importantly, they are not cumulative -- you shed your earlier privileges.
        Processes that will be handling sensitive info can run as weaker roles.
        (It would be even cooler if a process could downgrade its own role at runtime)

        <h3>AssumeRole makes CloudTrail logs confusing</h3>
        The thing I hate most about AWS CloudTrail and IAM...
        Is that when you assume a role and perform actions under that assumed role,
        your original identity is discarded in most stores and views.

        AWS pro tip:
        <a href="https://stackoverflow.com/a/62096435/190298">find the person who assumed a session</a>
        https://stackoverflow.com/questions/62096052/identify-aws-iam-user-that-assumed-an-iam-role/62096435#62096435

        <h3>Use MFA to Assume Roles</h3>
        As much as possible, you definitely want to require MFA for assuming roles
        Especially for use with an API key
        Because API keys are just too dangerous on their own, otherwise.
        This matters for things like Terraform.

        <h3>Key Pattern</h3>
        Some API keys follow a distinct format which allows us to do really cool stuff
        like this, without needing to buy GitGuardian or whatever. Find AWS API keys:
        <code>ag -lw 'AKIA[A-Z0-9]{16}'</code>

        <h3>aws-sdk and aws-cli don't load named profiles</h3>
        If you‚Äôre using aws-sdk, wanting to test locally,
        and using Named Profiles to assume IAM roles,
        then both these environment variables are required:
        AWS_SDK_LOAD_CONFIG=true AWS_PROFILE=your-whatever-profile

        AWS_SDK_LOAD_CONFIG says ‚Äúhey, use my ~/.aws/config instead of searching for http://169.254.169.254‚Äù
        AWS_PROFILE says which profile to load from there

        <h3>Wishlist: over-granted permissions</h3>
        Google Cloud has this very cool feature that shows a count for each user
        showing how many of the granted permissions are over-assigned (never used)
        This would be a super cool feature to have for AWS IAM that would help
        a lot in supporting least-privilege design.

        <a href="https://github.com/salesforce/policy_sentry">Policy Sentry</a> - IAM Least Privilege Policy Generator (by Salesforce)
    </section>

    <section>
        <h2>ChatBot</h2>
        You can use AWS ChatBot to pipe findings from GuardDuty directly into Slack. üòé

        It can also be set up to subscribe to messages that flow through SNS. Chatbot knows how to format messages from any of these origins:
        - Amazon CloudWatch Alarms Ôøº
        - Amazon CloudWatch Events (might be very cool üòé)
        - Notifications for AWS developer tools (????)
        - AWS CloudFormation ü§∑‚Äç‚ôÇÔ∏è
        - AWS Billing and Cost Management  üï¥Ô∏èüí∞

        <h3>Cannot send CloudWatch AlarmDescription</h3>
        I was experimenting with using AWS ChatBot to deliver CloudWatch alarms into Slack.
        Unfortunately, ChatBot does not forward the AlarmDescription.
        I like using AlarmDescription to include a runbook URL for each alarm.
        And the output from Chatbot cannot be customized at all.
    </section>

    <section>
        <h2>S3</h2>
        <h3>Use README files</h3>
        Put a README.txt file in the root of each bucket. Use this to express intent.
        <ul>
            <li>Are the contents of this bucket managed by human or machine?</li>
            <li>Are you expecting sensitive (PII, PHI, PCI) data in this bucket?</li>
            <li>How bad is it if all this data goes missing?</li>
        </ul>
        You'll thank yourself later.

        <h3>Surprising auto-decompression</h3>
        If you're using aws-sdk, or any software that uses aws-sdk, then any files that
        are compressed in AWS S3 will be automatically decompressed when you receive them.
        But headers and filenames remain unchanged. So you might think you have a compressed
        file, but actually you don't. And I don't think there's any way to retrieve the
        compressed representation using aws-sdk.

        I ran into this bug with Transmit.app for macOS -- files that are compressed in S3
        are downloaded to my Mac as uncompressed. I reported this bug to Panic.

        I also ran into this bug with Elastic FileBeat -- it crashed trying to ingest
        compressed files, because it attempted a second round of decompression.
        <a href="https://github.com/elastic/beats/issues/18696">
            Fails processing jsonl+gzip when using S3 Input plugin #18696
        </a>

        <h2>AWSLogs</h2>
        All the different possible methods of granting various AWS services the ability to write logs into your S3 bucket:
        - Create an IAM Role that can be assumed by the origin service and make sure it allows writing into the S3 bucket
        - Add an entry to the S3 Access Control List that allows an AWS-owned account to use the bucket (only CloudFront needs this)
        - Add a statement to S3 Bucket Policy that authorizes a specific origin service
        - Use a canned ACL to provide a special grant (needed for S3 server access logs)
        To collect logs from a variety of AWS services into one S3 bucket, all of these must be used together.
    </section>

    <section>
        <h2>EventBridge</h2>
        EventBridge is super cool, but I already found my first annoyance.
        <code>
            export interface SNSNotificationV1 {
            EventSource: 'aws:sns',
            ...

            export interface S3EventV2 {
            eventSource: 'aws:s3';
            ...
        </code>
        Why not use the same casing? Ôøº This is an AWS problem. I just had to write code to navigate around it.
    </section>

    <section>
        <h2>KMS</h2>
        KMS is just super hard, honestly. It'll get'ya every time.
        https://devops.stackexchange.com/q/11623/7770

        <h3>Avoid Orphaned Keys</h3>
        Beware the minimal required KMS key policy.
        Unlike other AWS services, KMS access is <mark>not automatically granted</mark> to any entity.
        So if you don't grant access to yourself, it becomes orphaned.
    </section>

    <section>
        <h2>CloudTrail</h2>
        CloudTrail is mostly useless for troubleshooting.
        First, because there's like a 5 minute delay for logs to arrive in CloudTrail.
        Second, because it has NEVER been able to answer my question of
        "why did I get AccessDenied on this API call?"
        Trying to use CloudTrail to write IAM policy is an exercise in futility.
    </section>

    <section>
        <h2>CloudFront</h2>
        <h3>Logs are Delayed</h3>
        There's always a 0-5 minute delay on ingesting CloudFront logs
        because they're written direct to S3 on an AWS-specified schedule.

        Proposed Alternative:
        If we really wanted to get those faster, we could generate our own logs with a Lambda@Edge function.

        <h3>No HSTS</h3>
        AWS CloudFront does not natively support addition of HSTS Strict-Transport-Security headers.

        Workaround:
        Looks like the only way to do this with AWS CloudFront is by using Lambda@Edge.
        https://forums.aws.amazon.com/thread.jspa?threadID=162252
        I still need to investigate the possible impact on performance and reliability.

        <h3>Multiple Distributions</h3>
        In many scenarios, you're presented with the option of creating multiple CloudFront
        distributions, or grouping the behaviour all into one big CF distribution.
        How does one decide which approach is better?
        <ul>
            <li>Billing tags are per-distribution.
                So if you want to do bandwidth chargebacks, you want a unique distribution for every customer.</li>
        </ul>

        <h3>Lambda@Edge functions are hard to delete</h3>
        <blockquote>
            Lambda was unable to delete arn:aws:lambda: ‚Ä¶ because it is a replicated function.
            Please see our documentation for Deleting Lambda@Edge Functions and Replicas.
        </blockquote>

        <h3>Random HTTP 500 errors with S3 origin</h3>
        <blockquote>
            During normal operations it is not unusual to encounter an occasional error from S3.
            <a href="https://forums.aws.amazon.com/message.jspa?messageID=708676#708676">(source)</a>
        </blockquote>

        <h3>40 Gbps</h3>
        CloudFront is effectively unlimited capacity for most people.
        If you expect to use more than 40 Gbps bandwidth (combined inbound + outbound)
        then you'll need to notify AWS Services Team about your planned load.

        More capacity notes:
        - Every CloudFront distribution is limited to 100,000 requests/second.
        - Billing is based on total bytes transferred out, so enabling compression can help to reduce costs in some cases.
    </section>

    <section>
        <h2>Elastic Container Service (ECS)</h2>
        <h3>IAM roles on Tasks, not Services</h3>
        Normally you want to assign IAM role to a Task, not a Service.

        ECS Services can carry IAM roles if AND ONLY IF they are also configured with a load balancer.

        <h3>Dangerous with auto-scaling groups</h3>
        Your services will get nuked without warning, ungracefully.
        The only solution I know of so far is to install a custom Lambda lifecycle event handler. Ôøº
        https://aws.amazon.com/blogs/compute/how-to-automate-container-instance-draining-in-amazon-ecs/

        <h3>OOM Killer</h3>
        ECS can easily kill your services and tasks if they surpass the hard memory limit, and
        if your monitoring is not good, you won't even know. Look for exit code 137 and
        "OutOfMemoryError: Container killed due to memory usage".

        <h3>Fatality</h3>
        Any ECS task that exits on its own will emit an event to EventBridge
        (aka CloudWatch Events) saying that it died, and the event doesn't carry the exit code.
        Any task that runs outside a service context is expected to exit 0,
        so you end up with false positive alerts saying that a service died, when it was expected.

        <h3>No metrics on non-service tasks</h3>
        Only ECS Services get CloudWatch metrics. If you want to know about the memory and CPU
        usage of a task that runs outside of a service context, you're shit outta luck.

        Workaround: include a sidecar container that polls the
        <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-metadata-endpoint-v3.html">
            task metadata endpoint
        </a>
        and emits metrics to statsd or graphite or datadog or whatever metrics tool you like.
        Heck, you could even send them to CloudWatch.

        <h3>Immutable Tags</h3>
        Immutable Tags seem like a good idea, but hard to use?
    </section>

    <section>
        <h2>Kinesis</h2>
        <h3>Data stuck in the stream</h3>
        There‚Äôs some data stuck in a Kinesis stream that‚Äôs going to expire in about 18 hours. I want it. It‚Äôs possible to re-read old events from a Kinesis stream, but‚Ä¶

        If you don‚Äôt know the exact time the events start, you need to slowly scroll forward from the oldest time available until you encounter records. Until you find the right place, it will just keep returning Records: []. This can take 20-60 round trips through the API. Ôøº  It‚Äôs a real, known issue. Actually it‚Äôs the ‚Äúexpected‚Äù behaviour.
        https://stackoverflow.com/a/38579528/190298
        https://forums.aws.amazon.com/thread.jspa?messageID=509980
    </section>

    <section>
        <h2>Kinesis Firehose</h2>
        <h3>missing newlines</h3>
        Objects will be dumped into S3 without any delimiters, so if inputs are lines of text
        or JSON objects, they'll be dumped all in a jumble.

        Workaround: make a processor Lambda function like kinesis-cloudwatch-newline.
        But beware the 6.00 MiB limit on Lambda response payloads!
    </section>

    <section>
        <h2>Parameter Store (SSM)</h2>
        The longest value that can be stored in AWS SSM Parameter Store is 4096 bytes.
    </section>

    <section>
        <h2>API Gateway</h2>
        Managing AWS API Gateway in AWS Console is still awful.
        API Gateway with Terraform is still unworkable.
        The only half-pleasant experience I've had with API Gateway was with the Serverless framework.

        <h3>HTTPS-only</h3>
        All APIs created with Amazon API Gateway expose HTTPS endpoints only.
        Amazon API Gateway does not support unencrypted (HTTP) endpoints.
        Source: https://aws.amazon.com/api-gateway/faqs/

        So... as long as I'm using API Gateway to front requests,
        I can't answer with plain HTTP on tcp/80.
        And some (most?) browsers don't automatically try tcp/443 if tcp/80 fails.

        <h3>Not good for redirects</h3>
        If you just want to make a super simple HTTP redirect for a vanity domain,
        API Gateway is way overkill and it's going to hurt you a lot.
        <a href="https://alestic.com/2015/11/amazon-api-gateway-aws-cli-redirect/">
            Creating An Amazon API Gateway With aws-cli For Domain Redirect
        </a>

        Workaround: S3 objects can carry redirect data, which works great if you have
        a lot of redirects in one domain. Still no great solution if you want to do
        redirects for a lot of different domains.
    </section>

    <section>
        <h2>Web App Firewall (WAF)</h2>
        WAF detected nothing abnormal during the pentest, and this result surprises me.
        We have it configured to detect and halt SQL injection attacks,
        and I expected the automated scanners used by our testers to trigger this rule.
        But it detected zero events during the testing period.
        The filter sensitivity is a lot lower than I expected.

        <h3>Two Worlds Apart</h3>
        AWS WAF (Web Application Firewall) introduced a new console.
        (Fair enough, AWS likes to do that from time to time.)
        But the new console doesn't show old resources. Old resources can only be seen in the old console. WTF.
    </section>

    <section>
        <h2>Elasticsearch Service</h2>
        The "Require HTTPS" feature for AWS Elasticsearch Service was only announced recently.
        I guess that explains why it's still poorly documented.
        whats-new/2019/10/amazon-elasticsearch-service-provides-option-to-mandate-https/

        Each Amazon ES domain resides within its own, dedicated VPC.
        This architecture prevents potential attackers from intercepting traffic between Elasticsearch nodes.
        So far so good. Seems reasonable.

        By default, however, traffic within the VPC is unencrypted.
        Node-to-node encryption enables TLS 1.2 encryption for all communications within the VPC.
        Okay that's an annoying default. It would be nice if AWS provided a rationale or shared the trade-offs.

        You can require that all traffic to the domain arrive over HTTPS using the console, aws-cli,
        or configuration API. (I would like to do that. Some details in the docs sure would be nice.)

        By default, domains do not use node-to-node encryption...
        and you can't configure existing domains to use the feature.
        To enable the feature, you must create another domain and migrate your data.
    </section>

    <section>
        <h2>Kinesis Firehose</h2>
        Shipping logs with ÓÄÄAWSÓÄÅ Kinesis Firehose is frustrating. It should be soso simple: receive log events from CloudWatch, write them to S3. Here are the ways in which Firehose is braindead:

        1. Records received from the Kinesis stream are concatenated without any record separator. This means that log messages, coming through as either text or JSON entities, get all jumbled together onto a single line making them nigh unreadable. It would be ideal if the record separator could be defined at configuration time, but alas that is not the case. The suggested workaround is to use a Lambda transform function.

        2. When using an S3 destination with Firehose, the filename and MIME type are dictated based on whether compression is applied after the transform. So if you want the file in S3 to end with .gz and have the correct MIME type for GZip, the only option is to enable post-transform compression (as we are doing). However, this means the protocol for the transform function becomes (input: Compressed) =&gt; output: Raw. That works fine for a while... until you run into the 6 MiB I/O limit for Lambda functions in synchronous execution mode. In this configuration, the input will eventually (perhaps often) be large enough that the output is larger than 6 MiB, so the transform fails and the logs are discarded.

        The only solution that solves all cases is a Lambda transform function that returns data up to 6 MB, then re-enqueues the rest back into the Kinesis/Firehose stream. This is a complex and delicate piece of code, which thankfully ÓÄÄAWSÓÄÅ is now providing as a Lambda blueprint.
    </section>

    <section>
        <h2>CloudWatch Alarms</h2>
        <h3>No dead-man switch</h3>
        Max alerting window is 24 hours.
        So you can't make a dead man switch (an alarm that triggers in the absence of activity)
        if the activity does not reliably occur within any 24 hour window.

        Alternatives: Elastalert? Datadog?
    </section>

    <section>
        <h2>Lambda</h2>
        <h3>Artifact is not run from S3</h3>
        AWS Lambda does not download a copy of the code from S3 every time a function is executed.
        Instead, Lambda has its own internal storage that is updated when invoking CreateFunction
        or UpdateFunctionCode either through the API or the Console.
        <a href="https://devops.stackexchange.com/a/6512/7770">devops.stackexchange.com</a>

        <h3>6.0 MiB limit on response payload</h3>
        AWS Lambda has a 6.00 <a href="https://en.wikipedia.org/wiki/Mebibyte">MiB</a> (6.29 MB)
        limit on size of the response payload for synchronous invocation.

        An additional 100 bytes are granted if your Lambda function returns an object,
        which brings the limit up to 6291556 bytes.

        This is particularly dangerous when writing a processor for Kinesis Firehose,
        because the input is compressed but the output must not be compressed.

        <a href="https://github.com/nhnicwaller/aws-lambda-limit">Limit details</a>
    </section>

    <section>
        <h2>Simple Notification Service (SNS)</h2>
        <h3>Raw message delivery</h3>
        Q: Can messages be forwarded from SNS without extra wrapping or message envelope?<br>
        A: Yes, by using "Enable raw message delivery" on the SNS topic subscription (only SQS or HTTP)

        <h3>No pub</h3>
    </section>

    <section>
        <h2>Athena</h2>
        https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/
    </section>

    <section>
        <a id="awslogs" href="#awslogs"><h2>Fragmented AWSLogs</h2></a>
        At first glance, it might seem like AWS has good support for collecting service logs and
        piping them to the log system of your choosing. CloudFront, EC2 load balancers, and S3
        buckets can all be configured to write their logs directly into an S3 bucket, and all
        use the same <code>AWSLogs</code> prefix and naming convention. Nice!

        But that's where the consistency stops and the pain starts.

        There are dozens of services that can't write their logs to S3. And some of the alternatives
        within the AWS ecosystem, CloudWatch Logs, SNS, and CloudTrail have even worse adoption.

        For the subset of AWS services that I use as a web developer, I need to use FIVE distinct
        systems of collecting logs. EventBridge (also called CloudWatch Events) handles around 50%
        of cases. The exceptions, those missing support for EventBridge, are detailed below grouped
        by the next-best option.

        <ul>
            <li>S3 (EC2 load balancer, CloudTrail management events, CloudFront, VPC flow logs, and S3 bucket server access logs)</li>
            <li>CloudWatch Log Group (Route53 query logs, Lambda invocations, ECS task logs)</li>
            <li>Email (SNS outputs, Certificate Manager, AWS Trusted Advisor)</li>
            <li>Kinesis Firehose (AWS WAF)</li>
        </ul>

        And then of course there's RDS error logs which can't be read from anywhere, other than by
        manually logging into the Console to read them yourself. Nice.
    </section>

    <section>
        <h1>See Also</h1>
        <a href="https://github.com/open-guides/og-aws/blob/master/README.md">Open Guide to AWS</a>
        <!-- TODO: cross-post some lessons to the open guide -->
        <a href="https://www.datacenterdynamics.com/en/news/wikileaks-publishes-list-aws-data-center-locations-colo-providers/">
            WikiLeaks publishes list of AWS data center locations, colo providers
        </a>
        <a href="https://github.com/toniblyx/my-arsenal-of-aws-security-tools">
            Arsenal of AWS security tools
        </a>
    </section>

</main>

<include src="/fragment/footer.html" />
</body>
</html>
